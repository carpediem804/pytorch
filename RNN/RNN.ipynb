{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNit+p1xW4QYfG/a4zAeJnM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"jyUnvIdEOI0G","colab_type":"code","colab":{}},"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchtext import data,datasets"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ze7JpP1-OvHH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"outputId":"ff19e1b2-dd62-4cb2-a3d5-74a0d96305b6","executionInfo":{"status":"ok","timestamp":1584343506500,"user_tz":-540,"elapsed":22783,"user":{"displayName":"민태홍","photoUrl":"","userId":"00517457566685021599"}}},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4B-s5JFfO18v","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"7843ab38-86b2-4e79-b311-c53dfd08a0e7","executionInfo":{"status":"ok","timestamp":1584343511151,"user_tz":-540,"elapsed":1647,"user":{"displayName":"민태홍","photoUrl":"","userId":"00517457566685021599"}}},"source":["USE_CUDA = torch.cuda.is_available()\n","DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n","DEVICE "],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"6ydKYPsVO3dy","colab_type":"code","colab":{}},"source":["## 하이퍼파라미터 설정\n","BATCH_SIZE = 64\n","lr = 0.001\n","EPOCHS = 50"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9FuxpKdvPAHw","colab_type":"code","colab":{}},"source":["## IMDB 데이터셋 로딩 후 텐서로 변환\n","TEXT = data.Field(sequential=True,batch_first=True,lower=True)\n","LABEL = data.Field(sequential=False,batch_first=True)\n","## sequential => 데이터가 순차적인 데이터셋인지 \n","## batch_first => 파라미터로 신경망에 입력되는 텐서의 첫번째 차원값이 batch_size가 되도록 \n","## lower => 텍스트 데이터 속 모든 영문 알파벳이 소문자가 되도록 \n","\n","trainset, testset = datasets.IMDB.splits(TEXT,LABEL)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NkUhdDEdQJC3","colab_type":"code","colab":{}},"source":["\n","## word embadding에 필요한 단어 사전을 만듬\n","\n","TEXT.build_vocab(trainset,min_freq=5)\n","LABEL.build_vocab(trainset)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q786ax6sQ0yn","colab_type":"code","colab":{}},"source":["## 검증 SET이 없어서 TRAINSET에서 VALID SET 나눠서 사용한다잉 ~ \n","trainset,valset = trainset.split(split_ratio=0.8)\n","\n","## batch 단위로 쪼개서 학습을 진행 한다 \n","## iter를 enumerate()함수에 입력시켜 루프를 구현하면 루프 때마다 전체 데이터셋에서 배치단위의 데이터가 생성 \n","train_iter,val_iter,test_iter = data.BucketIterator.splits(\n","    (trainset, valset,testset),\n","    batch_size =BATCH_SIZE,\n","    shuffle=True, repeat=False \n",")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6xJeMTxvR3ez","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"71d1ae67-8b2a-4545-f675-2f64d4070b6b","executionInfo":{"status":"ok","timestamp":1584348735231,"user_tz":-540,"elapsed":1669,"user":{"displayName":"민태홍","photoUrl":"","userId":"00517457566685021599"}}},"source":["## 사전 속 단어들으 ㅣ개수와 레이블으 ㅣ수를 정해주는 변수 \n","vocab_size = len(TEXT.vocab)\n","n_classes = 2 ## 1 or 2 임으로 \n","\n","print(\"[학습셋] : {} [valset] : {} [testset] : {} [단어수] : {} [class ]: {}\".format(len(trainset),len(valset),len(testset),vocab_size,n_classes))"],"execution_count":42,"outputs":[{"output_type":"stream","text":["[학습셋] : 20000 [valset] : 5000 [testset] : 25000 [단어수] : 46159 [class ]: 2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R2WbmAi2SrGq","colab_type":"code","colab":{}},"source":["## rnn 모델 구현 \n","\n","class basic_gru(nn.Module):\n","  def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n","    \n","    super(basic_gru,self).__init__()\n","    print(\"building basic gru model ~ \")\n","    ## n_layers : 은닉 벡터들의 층 \n","    self.n_layers = n_layers\n","   \n","    self.embed = nn.Embedding(n_vocab,embed_dim)\n","    ## nn.embedding(a,b) a:전체 데이터 셋의 모든 단어를 사전 형태로 나태냈을때 그 수 \n","    ## b: 임베딩된 단어가 갖는 차원 값 \n","   \n","    self.hidden_dim = hidden_dim\n","    self.dropout = nn.Dropout(dropout_p)\n","    self.gru = nn.GRU(embed_dim,self.hidden_dim,num_layers= self.n_layers,batch_first=True)\n","    ## nn.module에 있는 gru 가져다 쓰는것 \n","   \n","    self.output = nn.Linear(self.hidden_dim, n_classes)\n","\n","  def forward(self,x):\n","   \n","    x = self.embed(x)\n","    ##한 문장을 각 단어별로 임베딩 한 것 -> x \n","   \n","    ## 첫번째 은닉벡터 h0를 정으 ㅣ해 x와 함꼐 입력해줘야한다 \n","    h_0 = self._init_state(batch_size=x.size(0)) ##x의 첫번째 임베딩값을 넣는다 \n","   \n","    ##self.gru(x,h_0) => (batch_size, 입력x의 길이, hidden_dim)의 모양을 지닌 3d 텐서로 나옴 \n","    x,_ = self.gru(x,h_0)\n","    \n","    h_t = x[:,-1,:] ## -> 배치 내 모든 시계열 은닉 벡터들의 마지막 토큰들을 내포한 (batch_size,1,hidden_dim)의 텐서 추출  \n","    ## h_t == 영화 리뷰 배열들을 압축한 은닉 벡터 \n","   # print(\"h_t : {}\".format(h_t))\n","    self.dropout(h_t)\n","    \n","    logit = self.output(h_t) ## output == linear \n","    return logit\n","\n","  def _init_state(self,batch_size = 1):\n","    ## parameter함수는 nn.module의 가중치 정보들을 반복자 형태로 반환 \n","    ## 이 반복자가 생성하는 원소들은 각각 실제 신경망의 가중치 텐서(.data)를 지닌 객체들 \n","\n","    weight = next(self.parameters()).data ## -> nn.gru 모듈의 첫번째 가중치 텐서를 추출한다 \n","    ## new() 호출하여 모델으 ㅣ가중치와 같은 모양인 텐서로 바꾼후 zero_()를 호출하여 0으로 초기화 \n","    return weight.new(self.n_layers,batch_size,self.hidden_dim).zero_()\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bnrk41NvcgmJ","colab_type":"code","colab":{}},"source":["## train 과 evaluation \n","\n","def train(model, optimizer,train_iter):\n","  model.train()\n","  for b,batch in enumerate(train_iter):\n","    ##반복마다 배치데이터를 반환 \n","    x,y = batch.text.to(DEVICE),batch.label.to(DEVICE)\n","    y.data.sub_(1) ## label을 0~1로 변환 \n","\n","    ## 각 배치마다 기울기를 새로 계산 \n","    optimizer.zero_grad()\n","#    print(\"before train model \")\n","    logit = model(x)\n","#    print(\"after model \")\n","    loss = F.cross_entropy(logit,y) \n","    loss.backward() ##기울기 계산하고 ~ \n","    optimizer.step() ## 학습하고 ~~ \n","\n","\n","## valset과 testset의 성능 측정 \n","def evaluate(model,val_iter):\n","  model.eval()\n","  corrects, total_loss = 0,0\n","  for batch in val_iter:\n","    x,y = batch.text.to(DEVICE),batch.label.to(DEVICE)\n","    y.data.sub_(1)## label 0~1로 바꾸기 \n","\n","    logit = model(x)\n","    loss = F.cross_entropy(logit,y,reduction='sum') ## reduction = sum -> 오차의 합을 구함  \n","    total_loss += loss.item() ## 배치사이즈 만큼 데이터의 오류 다 합친거\n","    \n","    corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n","  \n","  size = len(val_iter.dataset)\n","  avg_loss = total_loss / size\n","  avg_accuracy = 100.0 * corrects/ size\n","  \n","  return avg_loss, avg_accuracy\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OtwatMpOfHSP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"271d2a3a-1886-426f-8cc1-01785bff6b20","executionInfo":{"status":"ok","timestamp":1584349452441,"user_tz":-540,"elapsed":1658,"user":{"displayName":"민태홍","photoUrl":"","userId":"00517457566685021599"}}},"source":["## 학습 전 모델 객체 정의 \n","## 은닉벡터 차원값 = 256 ,임베딩 토큰 차원값= 128 \n","model = basic_gru(n_layers=1,hidden_dim=256,n_vocab= vocab_size,embed_dim=128,n_classes=n_classes,dropout_p=0.5).to(DEVICE)\n","optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n"],"execution_count":73,"outputs":[{"output_type":"stream","text":["building basic gru model ~ \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WThyNaWqfuWn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":897},"outputId":"286e6a9d-252d-4868-84b2-654fc396aaa2","executionInfo":{"status":"ok","timestamp":1584352696044,"user_tz":-540,"elapsed":1608170,"user":{"displayName":"민태홍","photoUrl":"","userId":"00517457566685021599"}}},"source":["## 학습 실행 ㄱㄱ ~ \n","best_val_loss = None\n","checkpoint_dir = \"/content/gdrive/My Drive/Colab Notebooks/pytorch/RNN\"\n","\n","os.path.join(checkpoint_dir)\n","\n","for e in range(1,EPOCHS+1):\n","  train(model,optimizer,train_iter)\n","  val_loss, val_accuarcy = evaluate(model,val_iter)\n","\n","  print(\"epoch : {} val_loss : {} || val_accuarcy  {}\".format(e,val_loss,val_accuarcy))\n","\n","  ##목표 : 학습 오차가 아닌 검증 오차가 최소화된 모델 을 저장해야한다 \n","  if not best_val_loss or val_loss < best_val_loss :\n","    \n","    torch.save(model.state_dict(),checkpoint_dir+'/txtclassification.pt')\n","    best_val_loss = val_loss\n"],"execution_count":76,"outputs":[{"output_type":"stream","text":["epoch : 1 val_loss : 0.4356383192062378 || val_accuarcy  86.65999603271484\n","epoch : 2 val_loss : 0.43324092388153074 || val_accuarcy  87.37999725341797\n","epoch : 3 val_loss : 0.4510982828855514 || val_accuarcy  86.65999603271484\n","epoch : 4 val_loss : 0.45810309896469115 || val_accuarcy  87.07999420166016\n","epoch : 5 val_loss : 0.4826218725681305 || val_accuarcy  87.41999816894531\n","epoch : 6 val_loss : 0.4619948860168457 || val_accuarcy  87.54000091552734\n","epoch : 7 val_loss : 0.4785803104400635 || val_accuarcy  87.15999603271484\n","epoch : 8 val_loss : 0.4840695296764374 || val_accuarcy  86.95999908447266\n","epoch : 9 val_loss : 0.555582232284546 || val_accuarcy  87.5\n","epoch : 10 val_loss : 0.5797550442695618 || val_accuarcy  86.25999450683594\n","epoch : 11 val_loss : 0.5544119689941406 || val_accuarcy  86.5999984741211\n","epoch : 12 val_loss : 0.6303966052532196 || val_accuarcy  85.86000061035156\n","epoch : 13 val_loss : 0.6353427826881408 || val_accuarcy  85.86000061035156\n","epoch : 14 val_loss : 0.5316725960731506 || val_accuarcy  86.77999877929688\n","epoch : 15 val_loss : 0.5959381611824036 || val_accuarcy  86.57999420166016\n","epoch : 16 val_loss : 0.5882614098072052 || val_accuarcy  86.63999938964844\n","epoch : 17 val_loss : 0.6545459761619568 || val_accuarcy  85.77999877929688\n","epoch : 18 val_loss : 0.6357278417110444 || val_accuarcy  87.19999694824219\n","epoch : 19 val_loss : 0.6106379401683807 || val_accuarcy  87.1199951171875\n","epoch : 20 val_loss : 0.5993272232532502 || val_accuarcy  87.4000015258789\n","epoch : 21 val_loss : 0.6139293724775314 || val_accuarcy  87.27999877929688\n","epoch : 22 val_loss : 0.646862924528122 || val_accuarcy  87.22000122070312\n","epoch : 23 val_loss : 0.6549205424785614 || val_accuarcy  87.4000015258789\n","epoch : 24 val_loss : 0.6676062097549439 || val_accuarcy  87.1199951171875\n","epoch : 25 val_loss : 0.6517410965442657 || val_accuarcy  87.33999633789062\n","epoch : 26 val_loss : 0.6596042426586151 || val_accuarcy  87.37999725341797\n","epoch : 27 val_loss : 0.6576858573436737 || val_accuarcy  87.36000061035156\n","epoch : 28 val_loss : 0.5674774345636368 || val_accuarcy  86.5999984741211\n","epoch : 29 val_loss : 0.6143792287945747 || val_accuarcy  87.1199951171875\n","epoch : 30 val_loss : 0.5960122606277466 || val_accuarcy  87.29999542236328\n","epoch : 31 val_loss : 0.6632610667228699 || val_accuarcy  87.33999633789062\n","epoch : 32 val_loss : 0.6950655789852143 || val_accuarcy  87.19999694824219\n","epoch : 33 val_loss : 0.7101305520057678 || val_accuarcy  87.33999633789062\n","epoch : 34 val_loss : 0.7230757316112518 || val_accuarcy  87.43999481201172\n","epoch : 35 val_loss : 0.7364641400814056 || val_accuarcy  87.4000015258789\n","epoch : 36 val_loss : 0.7609194937705994 || val_accuarcy  86.75999450683594\n","epoch : 37 val_loss : 0.7578230971336365 || val_accuarcy  87.22000122070312\n","epoch : 38 val_loss : 0.7770931389808655 || val_accuarcy  87.19999694824219\n","epoch : 39 val_loss : 0.7832715569496155 || val_accuarcy  87.43999481201172\n","epoch : 40 val_loss : 0.8117124014854431 || val_accuarcy  87.33999633789062\n","epoch : 41 val_loss : 0.8171776945114135 || val_accuarcy  87.15999603271484\n","epoch : 42 val_loss : 0.8778373328208924 || val_accuarcy  87.23999786376953\n","epoch : 43 val_loss : 0.9008476099967957 || val_accuarcy  87.18000030517578\n","epoch : 44 val_loss : 0.9293170605659485 || val_accuarcy  87.23999786376953\n","epoch : 45 val_loss : 0.9759368865966797 || val_accuarcy  87.13999938964844\n","epoch : 46 val_loss : 1.0341408305168152 || val_accuarcy  86.91999816894531\n","epoch : 47 val_loss : 1.0365150917053223 || val_accuarcy  87.13999938964844\n","epoch : 48 val_loss : 1.0554709217071534 || val_accuarcy  87.18000030517578\n","epoch : 49 val_loss : 1.0658704187393189 || val_accuarcy  87.25999450683594\n","epoch : 50 val_loss : 1.0809411252975465 || val_accuarcy  87.23999786376953\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1xNnu-LZsCd3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"29870174-7af3-42e4-d539-dd52fb5e0920","executionInfo":{"status":"ok","timestamp":1584352704924,"user_tz":-540,"elapsed":5770,"user":{"displayName":"민태홍","photoUrl":"","userId":"00517457566685021599"}}},"source":["model.load_state_dict(torch.load(checkpoint_dir + '/txtclassification.pt'))\n","test_loss , test_acc = evaluate(model,test_iter)\n","print(\"test loss : {} test accuaracy : {}\".format(test_loss,test_acc))"],"execution_count":77,"outputs":[{"output_type":"stream","text":["test loss : 0.4165056896877289 test accuaracy : 86.51200103759766\n"],"name":"stdout"}]}]}