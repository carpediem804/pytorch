{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNOuezNHj33Q5wH3t4C4lnn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"7SSHPr3yTo08","colab_type":"code","colab":{}},"source":["import torch  \n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision import transforms, datasets"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SgG8bLct2UYk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"4bb9cf0a-338f-4ea5-c308-a860eeb1c5a7","executionInfo":{"status":"ok","timestamp":1579253677132,"user_tz":-540,"elapsed":2676,"user":{"displayName":"민태홍","photoUrl":"","userId":"00517457566685021599"}}},"source":["USE_CUDA = torch.cuda.is_available()\n","DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n","DEVICE "],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"trfviS2m2shE","colab_type":"code","colab":{}},"source":["## 하이퍼 파라미터 설정 \n","EPOCHS = 50\n","BATCH_SIZE = 64"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3A7mopPo21em","colab_type":"code","colab":{}},"source":["##fashion mnist 데이터 셋 불러오기 \n","image_download_url = \"/content/gdrive/My Drive/Colab Notebooks/pytorch/cnn\"\n","train_loader = torch.utils.data.DataLoader(\n","    datasets.FashionMNIST(\n","        root = image_download_url,\n","        train = True,\n","        download = True,\n","    \n","         transform=transforms.Compose([\n","                       transforms.ToTensor(),\n","                       transforms.Normalize((0.1307,), (0.3081,))\n","                   ])),\n","    batch_size = BATCH_SIZE,\n","    shuffle = True\n",")\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.FashionMNIST(\n","        root = image_download_url,\n","        train = False,\n","        transform=transforms.Compose([\n","                       transforms.ToTensor(),\n","                       transforms.Normalize((0.1307,), (0.3081,))\n","                   ])),\n","    batch_size=BATCH_SIZE,\n","    shuffle=True\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mCgqmgeK37Mm","colab_type":"code","colab":{}},"source":["class CNN(nn.Module):\n","  def __init__(self):\n","    super(CNN,self).__init__()\n","    self.conv1 = nn.Conv2d(in_channels=1,out_channels=10,kernel_size=5)\n","    self.conv2 = nn.Conv2d(in_channels=10,out_channels=20,kernel_size=5)\n","    self.drop = nn.Dropout2d() ## conv2 거치고 나온것 dropout하기 \n","    ## dropout해서 나온 것들 이제 일반 신경망 거치기 \n","    self.fc1 = nn.Linear(in_features=320,out_features=50)\n","    self.fc2 = nn.Linear(in_features=50,out_features=10) ## 최종 class 는 10개임으로 \n","  \n","  def forward(self,x):\n","    x = F.relu(F.max_pool2d(self.conv1(x),2)) ## 2== kernel 크기 \n","    x= F.relu(F.max_pool2d(self.conv2(x),2)) \n","\n","    ## 현재 x의 형태는 2차원 모양 \n","    x = x.view(-1,320) ## -1 -> 남은 차원 모두 , 320 = x가 가진 원소개수 \n","\n","    ##이제 1차원 특징들을 입력받아 class로 분류하는 신경망 구축 \n","    x = F.relu(self.fc1(x))\n","    x = F.dropout(x, training=self.training)\n","    x =self.fc2(x)\n","\n","    return F.log_softmax(input=x,dim=1)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ftmBp5096SSl","colab_type":"code","colab":{}},"source":["\n","model = CNN().to(DEVICE)\n","optimizer = optim.SGD(params=model.parameters(),lr=0.01,momentum=0.5)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x8_8OyLr66Xc","colab_type":"code","colab":{}},"source":["def train (model, train_loader,optimizer,epoch):\n","  model.train() ## 모델 트레인으로 바꾸고 \n","\n","  for batch_idx,(data,target) in enumerate(train_loader):\n","    data, target = data.to(DEVICE) ,target.to(DEVICE)## CUDA로 바꾸고 \n","  \n","    optimizer.zero_grad() ## 기울기 0으로 초기화 \n","    output = model(data)\n","    loss = F.cross_entropy(output,target)\n","    loss.backward()\n","    optimizer.step()\n","\n","    if(batch_idx %200 ==0):\n","      print('train epoch : {} [{}/{} ({:.0f}%)]\\tLoss : {:.6f}'.format(epoch,batch_idx*len(data),len(train_loader.dataset),100.*batch_idx / len(train_loader),loss.item()))\n","\n","\n","def evaluate(model, test_loader):\n","  model.eval()\n","  test_loss = 0\n","  correct = 0\n","  with torch.no_grad():##평가 과정에서는 기울기 계산 필요없음 \n","    for data, target in test_loader:\n","      data,target = data.to(DEVICE),target.to(DEVICE)\n","      output = model(data)\n","\n","      ## 배치 오차 합산 \n","      test_loss += F.cross_entropy(output,target,reduction='sum').item()\n","\n","      ## 가장 높은 값ㅇ르 가진 인덱스가 바로 예측 값 \n","      ## output.max() -> 2개의 값 출력 , 가장 큰 값과 인덱스 \n","      ## .eq() -> 값이 일치하는지 확인하는 것 \n","      pred = output.max(1,keepdim=True)[1]\n","      correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","  test_loss /= len(test_loader.dataset)\n","  test_accuracy = 100.*correct/len(test_loader.dataset)\n","  return test_loss, test_accuracy\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uQ9FuT-b9RR7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"8743334b-c417-4e27-ae93-e1c0c8d7b7c9","executionInfo":{"status":"ok","timestamp":1579259554594,"user_tz":-540,"elapsed":482578,"user":{"displayName":"민태홍","photoUrl":"","userId":"00517457566685021599"}}},"source":["for epoch in range(1,EPOCHS+1):\n","  train(model,train_loader,optimizer,epoch)\n","  test_loss ,test_accuracy = evaluate(model,test_loader)\n","  print('[{}] test loss : {:.4f}, accuracy : {:.2f}%'.format(epoch,test_loss,test_accuracy))"],"execution_count":39,"outputs":[{"output_type":"stream","text":["train epoch : 1 [0/60000 (0%)]\tLoss : 2.305283\n","train epoch : 1 [12800/60000 (21%)]\tLoss : 1.209217\n","train epoch : 1 [25600/60000 (43%)]\tLoss : 0.922607\n","train epoch : 1 [38400/60000 (64%)]\tLoss : 0.812042\n","train epoch : 1 [51200/60000 (85%)]\tLoss : 0.862138\n","[1] test loss : 0.6479, accuracy : 75.01%\n","train epoch : 2 [0/60000 (0%)]\tLoss : 0.797771\n","train epoch : 2 [12800/60000 (21%)]\tLoss : 0.963487\n","train epoch : 2 [25600/60000 (43%)]\tLoss : 0.704669\n","train epoch : 2 [38400/60000 (64%)]\tLoss : 0.676444\n","train epoch : 2 [51200/60000 (85%)]\tLoss : 0.630311\n","[2] test loss : 0.5503, accuracy : 78.72%\n","train epoch : 3 [0/60000 (0%)]\tLoss : 0.534949\n","train epoch : 3 [12800/60000 (21%)]\tLoss : 0.545371\n","train epoch : 3 [25600/60000 (43%)]\tLoss : 0.604474\n","train epoch : 3 [38400/60000 (64%)]\tLoss : 0.503509\n","train epoch : 3 [51200/60000 (85%)]\tLoss : 0.546364\n","[3] test loss : 0.5018, accuracy : 81.07%\n","train epoch : 4 [0/60000 (0%)]\tLoss : 0.692870\n","train epoch : 4 [12800/60000 (21%)]\tLoss : 0.661247\n","train epoch : 4 [25600/60000 (43%)]\tLoss : 0.605195\n","train epoch : 4 [38400/60000 (64%)]\tLoss : 0.591397\n","train epoch : 4 [51200/60000 (85%)]\tLoss : 0.530880\n","[4] test loss : 0.4734, accuracy : 82.40%\n","train epoch : 5 [0/60000 (0%)]\tLoss : 0.457680\n","train epoch : 5 [12800/60000 (21%)]\tLoss : 0.590780\n","train epoch : 5 [25600/60000 (43%)]\tLoss : 0.351997\n","train epoch : 5 [38400/60000 (64%)]\tLoss : 0.411609\n","train epoch : 5 [51200/60000 (85%)]\tLoss : 0.364543\n","[5] test loss : 0.4695, accuracy : 82.89%\n","train epoch : 6 [0/60000 (0%)]\tLoss : 0.380515\n","train epoch : 6 [12800/60000 (21%)]\tLoss : 0.375484\n","train epoch : 6 [25600/60000 (43%)]\tLoss : 0.453710\n","train epoch : 6 [38400/60000 (64%)]\tLoss : 0.500293\n","train epoch : 6 [51200/60000 (85%)]\tLoss : 0.415401\n","[6] test loss : 0.4278, accuracy : 84.10%\n","train epoch : 7 [0/60000 (0%)]\tLoss : 0.557419\n","train epoch : 7 [12800/60000 (21%)]\tLoss : 0.353069\n","train epoch : 7 [25600/60000 (43%)]\tLoss : 0.516968\n","train epoch : 7 [38400/60000 (64%)]\tLoss : 0.632839\n","train epoch : 7 [51200/60000 (85%)]\tLoss : 0.472212\n","[7] test loss : 0.4065, accuracy : 85.31%\n","train epoch : 8 [0/60000 (0%)]\tLoss : 0.372390\n","train epoch : 8 [12800/60000 (21%)]\tLoss : 0.354175\n","train epoch : 8 [25600/60000 (43%)]\tLoss : 0.411812\n","train epoch : 8 [38400/60000 (64%)]\tLoss : 0.367714\n","train epoch : 8 [51200/60000 (85%)]\tLoss : 0.549723\n","[8] test loss : 0.4018, accuracy : 85.18%\n","train epoch : 9 [0/60000 (0%)]\tLoss : 0.401457\n","train epoch : 9 [12800/60000 (21%)]\tLoss : 0.459223\n","train epoch : 9 [25600/60000 (43%)]\tLoss : 0.598858\n","train epoch : 9 [38400/60000 (64%)]\tLoss : 0.411267\n","train epoch : 9 [51200/60000 (85%)]\tLoss : 0.835102\n","[9] test loss : 0.3809, accuracy : 85.90%\n","train epoch : 10 [0/60000 (0%)]\tLoss : 0.641276\n","train epoch : 10 [12800/60000 (21%)]\tLoss : 0.390907\n","train epoch : 10 [25600/60000 (43%)]\tLoss : 0.495742\n","train epoch : 10 [38400/60000 (64%)]\tLoss : 0.363582\n","train epoch : 10 [51200/60000 (85%)]\tLoss : 0.549889\n","[10] test loss : 0.3729, accuracy : 86.68%\n","train epoch : 11 [0/60000 (0%)]\tLoss : 0.393371\n","train epoch : 11 [12800/60000 (21%)]\tLoss : 0.296923\n","train epoch : 11 [25600/60000 (43%)]\tLoss : 0.342847\n","train epoch : 11 [38400/60000 (64%)]\tLoss : 0.382819\n","train epoch : 11 [51200/60000 (85%)]\tLoss : 0.744078\n","[11] test loss : 0.3624, accuracy : 86.96%\n","train epoch : 12 [0/60000 (0%)]\tLoss : 0.506830\n","train epoch : 12 [12800/60000 (21%)]\tLoss : 0.445925\n","train epoch : 12 [25600/60000 (43%)]\tLoss : 0.358279\n","train epoch : 12 [38400/60000 (64%)]\tLoss : 0.242527\n","train epoch : 12 [51200/60000 (85%)]\tLoss : 0.264722\n","[12] test loss : 0.3632, accuracy : 86.72%\n","train epoch : 13 [0/60000 (0%)]\tLoss : 0.445560\n","train epoch : 13 [12800/60000 (21%)]\tLoss : 0.685995\n","train epoch : 13 [25600/60000 (43%)]\tLoss : 0.274183\n","train epoch : 13 [38400/60000 (64%)]\tLoss : 0.290811\n","train epoch : 13 [51200/60000 (85%)]\tLoss : 0.346028\n","[13] test loss : 0.3554, accuracy : 87.19%\n","train epoch : 14 [0/60000 (0%)]\tLoss : 0.272010\n","train epoch : 14 [12800/60000 (21%)]\tLoss : 0.677719\n","train epoch : 14 [25600/60000 (43%)]\tLoss : 0.475646\n","train epoch : 14 [38400/60000 (64%)]\tLoss : 0.547523\n","train epoch : 14 [51200/60000 (85%)]\tLoss : 0.318593\n","[14] test loss : 0.3558, accuracy : 86.85%\n","train epoch : 15 [0/60000 (0%)]\tLoss : 0.463817\n","train epoch : 15 [12800/60000 (21%)]\tLoss : 0.420958\n","train epoch : 15 [25600/60000 (43%)]\tLoss : 0.366244\n","train epoch : 15 [38400/60000 (64%)]\tLoss : 0.461786\n","train epoch : 15 [51200/60000 (85%)]\tLoss : 0.354519\n","[15] test loss : 0.3482, accuracy : 87.33%\n","train epoch : 16 [0/60000 (0%)]\tLoss : 0.423249\n","train epoch : 16 [12800/60000 (21%)]\tLoss : 0.252173\n","train epoch : 16 [25600/60000 (43%)]\tLoss : 0.357012\n","train epoch : 16 [38400/60000 (64%)]\tLoss : 0.312709\n","train epoch : 16 [51200/60000 (85%)]\tLoss : 0.311177\n","[16] test loss : 0.3542, accuracy : 87.10%\n","train epoch : 17 [0/60000 (0%)]\tLoss : 0.339811\n","train epoch : 17 [12800/60000 (21%)]\tLoss : 0.448711\n","train epoch : 17 [25600/60000 (43%)]\tLoss : 0.304072\n","train epoch : 17 [38400/60000 (64%)]\tLoss : 0.522023\n","train epoch : 17 [51200/60000 (85%)]\tLoss : 0.400650\n","[17] test loss : 0.3433, accuracy : 87.67%\n","train epoch : 18 [0/60000 (0%)]\tLoss : 0.406817\n","train epoch : 18 [12800/60000 (21%)]\tLoss : 0.338833\n","train epoch : 18 [25600/60000 (43%)]\tLoss : 0.340998\n","train epoch : 18 [38400/60000 (64%)]\tLoss : 0.524516\n","train epoch : 18 [51200/60000 (85%)]\tLoss : 0.536911\n","[18] test loss : 0.3387, accuracy : 87.61%\n","train epoch : 19 [0/60000 (0%)]\tLoss : 0.297790\n","train epoch : 19 [12800/60000 (21%)]\tLoss : 0.266761\n","train epoch : 19 [25600/60000 (43%)]\tLoss : 0.470390\n","train epoch : 19 [38400/60000 (64%)]\tLoss : 0.407794\n","train epoch : 19 [51200/60000 (85%)]\tLoss : 0.403490\n","[19] test loss : 0.3308, accuracy : 88.12%\n","train epoch : 20 [0/60000 (0%)]\tLoss : 0.485478\n","train epoch : 20 [12800/60000 (21%)]\tLoss : 0.373485\n","train epoch : 20 [25600/60000 (43%)]\tLoss : 0.394509\n","train epoch : 20 [38400/60000 (64%)]\tLoss : 0.437480\n","train epoch : 20 [51200/60000 (85%)]\tLoss : 0.319633\n","[20] test loss : 0.3298, accuracy : 87.71%\n","train epoch : 21 [0/60000 (0%)]\tLoss : 0.464092\n","train epoch : 21 [12800/60000 (21%)]\tLoss : 0.261888\n","train epoch : 21 [25600/60000 (43%)]\tLoss : 0.270543\n","train epoch : 21 [38400/60000 (64%)]\tLoss : 0.199369\n","train epoch : 21 [51200/60000 (85%)]\tLoss : 0.422851\n","[21] test loss : 0.3269, accuracy : 88.32%\n","train epoch : 22 [0/60000 (0%)]\tLoss : 0.205758\n","train epoch : 22 [12800/60000 (21%)]\tLoss : 0.353934\n","train epoch : 22 [25600/60000 (43%)]\tLoss : 0.319044\n","train epoch : 22 [38400/60000 (64%)]\tLoss : 0.223310\n","train epoch : 22 [51200/60000 (85%)]\tLoss : 0.300120\n","[22] test loss : 0.3225, accuracy : 88.49%\n","train epoch : 23 [0/60000 (0%)]\tLoss : 0.232340\n","train epoch : 23 [12800/60000 (21%)]\tLoss : 0.377160\n","train epoch : 23 [25600/60000 (43%)]\tLoss : 0.276560\n","train epoch : 23 [38400/60000 (64%)]\tLoss : 0.305820\n","train epoch : 23 [51200/60000 (85%)]\tLoss : 0.417155\n","[23] test loss : 0.3182, accuracy : 88.45%\n","train epoch : 24 [0/60000 (0%)]\tLoss : 0.262135\n","train epoch : 24 [12800/60000 (21%)]\tLoss : 0.324973\n","train epoch : 24 [25600/60000 (43%)]\tLoss : 0.298406\n","train epoch : 24 [38400/60000 (64%)]\tLoss : 0.309420\n","train epoch : 24 [51200/60000 (85%)]\tLoss : 0.257595\n","[24] test loss : 0.3199, accuracy : 88.82%\n","train epoch : 25 [0/60000 (0%)]\tLoss : 0.399180\n","train epoch : 25 [12800/60000 (21%)]\tLoss : 0.182424\n","train epoch : 25 [25600/60000 (43%)]\tLoss : 0.249312\n","train epoch : 25 [38400/60000 (64%)]\tLoss : 0.356252\n","train epoch : 25 [51200/60000 (85%)]\tLoss : 0.257201\n","[25] test loss : 0.3191, accuracy : 88.65%\n","train epoch : 26 [0/60000 (0%)]\tLoss : 0.245413\n","train epoch : 26 [12800/60000 (21%)]\tLoss : 0.395714\n","train epoch : 26 [25600/60000 (43%)]\tLoss : 0.173159\n","train epoch : 26 [38400/60000 (64%)]\tLoss : 0.282256\n","train epoch : 26 [51200/60000 (85%)]\tLoss : 0.397492\n","[26] test loss : 0.3226, accuracy : 88.46%\n","train epoch : 27 [0/60000 (0%)]\tLoss : 0.461899\n","train epoch : 27 [12800/60000 (21%)]\tLoss : 0.201883\n","train epoch : 27 [25600/60000 (43%)]\tLoss : 0.266988\n","train epoch : 27 [38400/60000 (64%)]\tLoss : 0.323590\n","train epoch : 27 [51200/60000 (85%)]\tLoss : 0.470724\n","[27] test loss : 0.3116, accuracy : 88.45%\n","train epoch : 28 [0/60000 (0%)]\tLoss : 0.246646\n","train epoch : 28 [12800/60000 (21%)]\tLoss : 0.485723\n","train epoch : 28 [25600/60000 (43%)]\tLoss : 0.271988\n","train epoch : 28 [38400/60000 (64%)]\tLoss : 0.286905\n","train epoch : 28 [51200/60000 (85%)]\tLoss : 0.373920\n","[28] test loss : 0.3203, accuracy : 88.40%\n","train epoch : 29 [0/60000 (0%)]\tLoss : 0.331033\n","train epoch : 29 [12800/60000 (21%)]\tLoss : 0.260800\n","train epoch : 29 [25600/60000 (43%)]\tLoss : 0.389014\n","train epoch : 29 [38400/60000 (64%)]\tLoss : 0.320649\n","train epoch : 29 [51200/60000 (85%)]\tLoss : 0.258257\n","[29] test loss : 0.3224, accuracy : 88.37%\n","train epoch : 30 [0/60000 (0%)]\tLoss : 0.328240\n","train epoch : 30 [12800/60000 (21%)]\tLoss : 0.319332\n","train epoch : 30 [25600/60000 (43%)]\tLoss : 0.415877\n","train epoch : 30 [38400/60000 (64%)]\tLoss : 0.256894\n","train epoch : 30 [51200/60000 (85%)]\tLoss : 0.399056\n","[30] test loss : 0.3156, accuracy : 88.61%\n","train epoch : 31 [0/60000 (0%)]\tLoss : 0.254114\n","train epoch : 31 [12800/60000 (21%)]\tLoss : 0.299499\n","train epoch : 31 [25600/60000 (43%)]\tLoss : 0.415104\n","train epoch : 31 [38400/60000 (64%)]\tLoss : 0.162797\n","train epoch : 31 [51200/60000 (85%)]\tLoss : 0.199395\n","[31] test loss : 0.3145, accuracy : 88.85%\n","train epoch : 32 [0/60000 (0%)]\tLoss : 0.407951\n","train epoch : 32 [12800/60000 (21%)]\tLoss : 0.300894\n","train epoch : 32 [25600/60000 (43%)]\tLoss : 0.327818\n","train epoch : 32 [38400/60000 (64%)]\tLoss : 0.261661\n","train epoch : 32 [51200/60000 (85%)]\tLoss : 0.506021\n","[32] test loss : 0.3053, accuracy : 89.10%\n","train epoch : 33 [0/60000 (0%)]\tLoss : 0.261182\n","train epoch : 33 [12800/60000 (21%)]\tLoss : 0.295688\n","train epoch : 33 [25600/60000 (43%)]\tLoss : 0.337941\n","train epoch : 33 [38400/60000 (64%)]\tLoss : 0.388592\n","train epoch : 33 [51200/60000 (85%)]\tLoss : 0.292908\n","[33] test loss : 0.3124, accuracy : 88.90%\n","train epoch : 34 [0/60000 (0%)]\tLoss : 0.482258\n","train epoch : 34 [12800/60000 (21%)]\tLoss : 0.242267\n","train epoch : 34 [25600/60000 (43%)]\tLoss : 0.391469\n","train epoch : 34 [38400/60000 (64%)]\tLoss : 0.165039\n","train epoch : 34 [51200/60000 (85%)]\tLoss : 0.325875\n","[34] test loss : 0.3142, accuracy : 88.79%\n","train epoch : 35 [0/60000 (0%)]\tLoss : 0.404047\n","train epoch : 35 [12800/60000 (21%)]\tLoss : 0.419356\n","train epoch : 35 [25600/60000 (43%)]\tLoss : 0.267372\n","train epoch : 35 [38400/60000 (64%)]\tLoss : 0.235078\n","train epoch : 35 [51200/60000 (85%)]\tLoss : 0.202512\n","[35] test loss : 0.3165, accuracy : 88.65%\n","train epoch : 36 [0/60000 (0%)]\tLoss : 0.484068\n","train epoch : 36 [12800/60000 (21%)]\tLoss : 0.290011\n","train epoch : 36 [25600/60000 (43%)]\tLoss : 0.307614\n","train epoch : 36 [38400/60000 (64%)]\tLoss : 0.165983\n","train epoch : 36 [51200/60000 (85%)]\tLoss : 0.397882\n","[36] test loss : 0.3097, accuracy : 89.01%\n","train epoch : 37 [0/60000 (0%)]\tLoss : 0.326194\n","train epoch : 37 [12800/60000 (21%)]\tLoss : 0.240394\n","train epoch : 37 [25600/60000 (43%)]\tLoss : 0.343217\n","train epoch : 37 [38400/60000 (64%)]\tLoss : 0.358146\n","train epoch : 37 [51200/60000 (85%)]\tLoss : 0.272079\n","[37] test loss : 0.3023, accuracy : 89.41%\n","train epoch : 38 [0/60000 (0%)]\tLoss : 0.126962\n","train epoch : 38 [12800/60000 (21%)]\tLoss : 0.230489\n","train epoch : 38 [25600/60000 (43%)]\tLoss : 0.373436\n","train epoch : 38 [38400/60000 (64%)]\tLoss : 0.456317\n","train epoch : 38 [51200/60000 (85%)]\tLoss : 0.302552\n","[38] test loss : 0.3112, accuracy : 88.74%\n","train epoch : 39 [0/60000 (0%)]\tLoss : 0.283564\n","train epoch : 39 [12800/60000 (21%)]\tLoss : 0.361829\n","train epoch : 39 [25600/60000 (43%)]\tLoss : 0.468645\n","train epoch : 39 [38400/60000 (64%)]\tLoss : 0.282267\n","train epoch : 39 [51200/60000 (85%)]\tLoss : 0.221329\n","[39] test loss : 0.3094, accuracy : 88.88%\n","train epoch : 40 [0/60000 (0%)]\tLoss : 0.207498\n","train epoch : 40 [12800/60000 (21%)]\tLoss : 0.180706\n","train epoch : 40 [25600/60000 (43%)]\tLoss : 0.278098\n","train epoch : 40 [38400/60000 (64%)]\tLoss : 0.532916\n","train epoch : 40 [51200/60000 (85%)]\tLoss : 0.382230\n","[40] test loss : 0.3094, accuracy : 89.32%\n","train epoch : 41 [0/60000 (0%)]\tLoss : 0.154734\n","train epoch : 41 [12800/60000 (21%)]\tLoss : 0.250941\n","train epoch : 41 [25600/60000 (43%)]\tLoss : 0.388921\n","train epoch : 41 [38400/60000 (64%)]\tLoss : 0.196030\n","train epoch : 41 [51200/60000 (85%)]\tLoss : 0.493636\n","[41] test loss : 0.3018, accuracy : 89.30%\n","train epoch : 42 [0/60000 (0%)]\tLoss : 0.267181\n","train epoch : 42 [12800/60000 (21%)]\tLoss : 0.387789\n","train epoch : 42 [25600/60000 (43%)]\tLoss : 0.251344\n","train epoch : 42 [38400/60000 (64%)]\tLoss : 0.311251\n","train epoch : 42 [51200/60000 (85%)]\tLoss : 0.306820\n","[42] test loss : 0.3018, accuracy : 89.38%\n","train epoch : 43 [0/60000 (0%)]\tLoss : 0.304086\n","train epoch : 43 [12800/60000 (21%)]\tLoss : 0.572797\n","train epoch : 43 [25600/60000 (43%)]\tLoss : 0.210094\n","train epoch : 43 [38400/60000 (64%)]\tLoss : 0.181890\n","train epoch : 43 [51200/60000 (85%)]\tLoss : 0.295455\n","[43] test loss : 0.3064, accuracy : 89.34%\n","train epoch : 44 [0/60000 (0%)]\tLoss : 0.299811\n","train epoch : 44 [12800/60000 (21%)]\tLoss : 0.224682\n","train epoch : 44 [25600/60000 (43%)]\tLoss : 0.321430\n","train epoch : 44 [38400/60000 (64%)]\tLoss : 0.368386\n","train epoch : 44 [51200/60000 (85%)]\tLoss : 0.145591\n","[44] test loss : 0.3113, accuracy : 89.08%\n","train epoch : 45 [0/60000 (0%)]\tLoss : 0.347119\n","train epoch : 45 [12800/60000 (21%)]\tLoss : 0.345283\n","train epoch : 45 [25600/60000 (43%)]\tLoss : 0.334762\n","train epoch : 45 [38400/60000 (64%)]\tLoss : 0.537128\n","train epoch : 45 [51200/60000 (85%)]\tLoss : 0.528038\n","[45] test loss : 0.3085, accuracy : 89.10%\n","train epoch : 46 [0/60000 (0%)]\tLoss : 0.546648\n","train epoch : 46 [12800/60000 (21%)]\tLoss : 0.230252\n","train epoch : 46 [25600/60000 (43%)]\tLoss : 0.225553\n","train epoch : 46 [38400/60000 (64%)]\tLoss : 0.190867\n","train epoch : 46 [51200/60000 (85%)]\tLoss : 0.228595\n","[46] test loss : 0.3032, accuracy : 89.71%\n","train epoch : 47 [0/60000 (0%)]\tLoss : 0.273656\n","train epoch : 47 [12800/60000 (21%)]\tLoss : 0.456394\n","train epoch : 47 [25600/60000 (43%)]\tLoss : 0.339507\n","train epoch : 47 [38400/60000 (64%)]\tLoss : 0.354361\n","train epoch : 47 [51200/60000 (85%)]\tLoss : 0.280355\n","[47] test loss : 0.3011, accuracy : 89.19%\n","train epoch : 48 [0/60000 (0%)]\tLoss : 0.286231\n","train epoch : 48 [12800/60000 (21%)]\tLoss : 0.265387\n","train epoch : 48 [25600/60000 (43%)]\tLoss : 0.234440\n","train epoch : 48 [38400/60000 (64%)]\tLoss : 0.184814\n","train epoch : 48 [51200/60000 (85%)]\tLoss : 0.332433\n","[48] test loss : 0.3039, accuracy : 89.53%\n","train epoch : 49 [0/60000 (0%)]\tLoss : 0.213243\n","train epoch : 49 [12800/60000 (21%)]\tLoss : 0.353732\n","train epoch : 49 [25600/60000 (43%)]\tLoss : 0.499811\n","train epoch : 49 [38400/60000 (64%)]\tLoss : 0.402740\n","train epoch : 49 [51200/60000 (85%)]\tLoss : 0.188933\n","[49] test loss : 0.3093, accuracy : 89.24%\n","train epoch : 50 [0/60000 (0%)]\tLoss : 0.295385\n","train epoch : 50 [12800/60000 (21%)]\tLoss : 0.315894\n","train epoch : 50 [25600/60000 (43%)]\tLoss : 0.131896\n","train epoch : 50 [38400/60000 (64%)]\tLoss : 0.268345\n","train epoch : 50 [51200/60000 (85%)]\tLoss : 0.438022\n","[50] test loss : 0.3083, accuracy : 89.50%\n"],"name":"stdout"}]}]}