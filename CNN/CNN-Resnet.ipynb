{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN-Resnet.ipynb","provenance":[],"authorship_tag":"ABX9TyMsZNHoVBXV1lT55aqs/Ol/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"XTEb-qXEgMiv","colab_type":"code","colab":{}},"source":["import torch  \n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision import transforms, datasets, models"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uoO6PbXehe7l","colab_type":"code","outputId":"fc68a836-d870-4292-f04b-6d79111ee159","executionInfo":{"status":"ok","timestamp":1581322228754,"user_tz":-540,"elapsed":21965,"user":{"displayName":"민태홍","photoUrl":"","userId":"00517457566685021599"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GM0_-_5EgUKL","colab_type":"code","outputId":"1898ba20-424b-4a57-e062-965cd58c09d7","executionInfo":{"status":"ok","timestamp":1581322236274,"user_tz":-540,"elapsed":845,"user":{"displayName":"민태홍","photoUrl":"","userId":"00517457566685021599"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["USE_CUDA = torch.cuda.is_available()\n","DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n","DEVICE "],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"7ouiC2yugwAp","colab_type":"code","colab":{}},"source":["EPOCH = 300\n","BATCH_SIZE = 128\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xHT_nqnShBY4","colab_type":"code","outputId":"98a17dfd-d7dc-4296-d910-064047842c7e","executionInfo":{"status":"ok","timestamp":1581322249268,"user_tz":-540,"elapsed":6083,"user":{"displayName":"민태홍","photoUrl":"","userId":"00517457566685021599"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#CIFAR 10 Dataset 사용 \n","image_download_url = \"/content/gdrive/My Drive/Colab Notebooks/pytorch/cnn\"\n","\n","train_loader = torch.utils.data.DataLoader(\n","    datasets.CIFAR10(\n","        root = image_download_url,\n","        train=True,\n","        download = True,\n","        transform = transforms.Compose([\n","                                        transforms.RandomCrop(32,padding=4),\n","                                        transforms.RandomHorizontalFlip(),\n","                                        transforms.ToTensor(),\n","                                        transforms.Normalize((0.5,0.5,0.5),\n","                                                             (0.5,0.5,0.5))\n","        ])), ## dataset 그거까지\n","        batch_size = BATCH_SIZE,\n","        shuffle=True\n",")\n","\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.CIFAR10(\n","        root = image_download_url,\n","        train=False,\n","        transform=transforms.Compose([\n","                                      transforms.ToTensor(),\n","                                      transforms.Normalize((0.5,0.5,0.5)\n","                                      ,(0.5,0.5,0.5))\n","        ])\n","    ),\n","    batch_size=BATCH_SIZE,\n","    shuffle=True\n",")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3iM1xO6pjDfE","colab_type":"code","colab":{}},"source":["class BasicBlock(nn.Module):\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n","                               stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PmBSuwcns1cG","colab_type":"code","colab":{}},"source":["## 모델정의 \n","## 이미지 -> 컨볼루션 + 배치정규화 -> basicblock 통과하고 -> 평균 풀링과 신경망 거쳐 예측 출력 \n","## basicblock = self.make_layer() 함수를 통해 하나의 모듈로 객체화 된다 \n","\n","class ResNet(nn.Module):\n","  def __init__(self,num_classes = 10):\n","    super(ResNet, self).__init__()\n","\n","    self.in_planes = 16 #layer1 층이 입력받는 채널의 개수가 16이여서 16으로 초기화 \n","\n","    self.conv1 = nn.Conv2d(3,16,kernel_size=3,stride=1,padding=1,bias=False) ## -> 통과후 16*32*32\n","    self.bn1 = nn.BatchNorm2d(16)\n","    self.layer1 = self._make_layer(16,2,stride=1) ## 16*32*32\n","    self.layer2 = self._make_layer(32,2,stride=2) ## 32*16*16\n","    self.layer3 = self._make_layer(64,2,stride=2) ## 64*8*8\n","    self.linear = nn.Linear(64,out_features=num_classes)\n","\n","  def _make_layer(self, planes, num_block, stride):\n","    print(\"in_make_layer\")\n","    strides = [stride] + [1]*(num_block-1) ## ??뭐지 이거 \n","    print(\"planes  {} num_block : {} stride : {}\".format(planes,num_block,stride))\n","    layers = []\n","\n","    for stride in strides : \n","      layers.append(BasicBlock(in_planes=self.in_planes,planes=planes,stride=stride))\n","      self.in_planes = planes\n","    \n","    return nn.Sequential(*layers)\n","  \n","  def forward(self,x):\n","    #print(\"in_Restent\")\n","    out = F.relu(self.bn1(self.conv1(x)))\n","    #print(\"first _finish\")\n","    out = self.layer1(out)\n","    #print(\"layer1\")\n","    out = self.layer2(out)\n","    #print(\"layer2\")\n","    out = self.layer3(out)\n","    #print(\"layter3\")\n","    out = F.avg_pool2d(out,8)\n","    out = out.view(out.size(0),-1)##1차원으로 쫙 피고\n","\n","    out = self.linear(out)\n","\n","    return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wm5AvesMxj45","colab_type":"code","outputId":"a8465cfa-a1a4-45f0-c5b5-8c310851224f","executionInfo":{"status":"ok","timestamp":1581322738672,"user_tz":-540,"elapsed":579,"user":{"displayName":"민태홍","photoUrl":"","userId":"00517457566685021599"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model = ResNet().to(DEVICE) ## GPU로 보내고 \n","optimizer = optim.SGD(model.parameters(),lr=0.1,momentum=0.9,weight_decay=0.0005)\n","scheduler = optim.lr_scheduler.StepLR(optimizer=optimizer,step_size=50,gamma=0.1)\n","##optim.lr_scheduler.StepLR == 학습이 진행되면서 학습률을 점점 낮춰서 정교하게 최적화 하는것 \n","## step_size만큼 호출 되었을 때 학습률에 0.1값을 곱한다 \n","print(model)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["in_make_layer\n","planes  16 num_block : 2 stride : 1\n","in_make_layer\n","planes  32 num_block : 2 stride : 2\n","in_make_layer\n","planes  64 num_block : 2 stride : 2\n","ResNet(\n","  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential(\n","        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential(\n","        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (linear): Linear(in_features=64, out_features=10, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WFJtph_V0mcF","colab_type":"code","colab":{}},"source":["def train(model, train_loader, optimizer, epoch):\n","  model.train()\n","  for batch_idx, (data, target) in enumerate(train_loader):\n","      data, target = data.to(DEVICE), target.to(DEVICE)\n","      optimizer.zero_grad()\n","      output = model(data)\n","      loss = F.cross_entropy(output, target)\n","      loss.backward()\n","      optimizer.step()\n","\n","def evaluate(model, test_loader):\n","  model.eval()\n","  test_loss = 0\n","  correct = 0\n","  with torch.no_grad():##평가 과정에서는 기울기 계산 필요없음 \n","    for data, target in test_loader:\n","      data,target = data.to(DEVICE),target.to(DEVICE)\n","      output = model(data)\n","\n","      ## 배치 오차 합산 \n","      test_loss += F.cross_entropy(output,target,reduction='sum').item()\n","\n","      ## 가장 높은 값ㅇ르 가진 인덱스가 바로 예측 값 \n","      ## output.max() -> 2개의 값 출력 , 가장 큰 값과 인덱스 \n","      ## .eq() -> 값이 일치하는지 확인하는 것 \n","      pred = output.max(1,keepdim=True)[1]\n","      correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","  test_loss /= len(test_loader.dataset)\n","  test_accuracy = 100.*correct/len(test_loader.dataset)\n","  return test_loss, test_accuracy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T93L9EJfz_a-","colab_type":"code","outputId":"4e1b5830-ba35-41ad-ae29-d57454d9e4d1","executionInfo":{"status":"error","timestamp":1580546289150,"user_tz":-540,"elapsed":549,"user":{"displayName":"민태홍","photoUrl":"","userId":"00517457566685021599"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["for epoch in range(1,EPOCH+1):\n","  scheduler.step() ##학습률을 조금 낮춰주는 단계\n","  \n","  train(model,train_loader,optimizer,epoch)\n","  test_loss ,test_accuracy = evaluate(model,test_loader)\n","  print('[{}] test loss : {:.4f}, accuracy : {:.2f}%'.format(epoch,test_loss,test_accuracy))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[1] test loss : 1.4112, accuracy : 49.11%\n","[2] test loss : 1.1272, accuracy : 61.18%\n","[3] test loss : 1.0150, accuracy : 64.47%\n","[4] test loss : 0.9553, accuracy : 68.12%\n","[5] test loss : 0.8945, accuracy : 69.20%\n","[6] test loss : 0.9682, accuracy : 69.68%\n","[7] test loss : 0.8160, accuracy : 72.34%\n","[8] test loss : 0.9641, accuracy : 70.25%\n","[9] test loss : 0.7171, accuracy : 75.15%\n","[10] test loss : 0.8435, accuracy : 72.20%\n","[11] test loss : 0.6762, accuracy : 77.05%\n","[12] test loss : 0.7674, accuracy : 75.19%\n","[13] test loss : 0.9534, accuracy : 70.22%\n","[14] test loss : 1.0972, accuracy : 68.34%\n","[15] test loss : 0.7396, accuracy : 75.57%\n","[16] test loss : 0.8477, accuracy : 73.27%\n","[17] test loss : 0.6437, accuracy : 77.42%\n","[18] test loss : 0.6263, accuracy : 78.33%\n","[19] test loss : 0.8175, accuracy : 73.95%\n","[20] test loss : 0.6596, accuracy : 77.49%\n","[21] test loss : 0.8940, accuracy : 73.01%\n","[22] test loss : 1.2606, accuracy : 65.12%\n","[23] test loss : 0.8182, accuracy : 74.79%\n","[24] test loss : 1.0803, accuracy : 68.40%\n","[25] test loss : 0.8380, accuracy : 72.57%\n","[26] test loss : 0.7501, accuracy : 75.85%\n","[27] test loss : 0.7910, accuracy : 74.76%\n","[28] test loss : 1.1085, accuracy : 68.19%\n","[29] test loss : 0.9994, accuracy : 68.16%\n","[30] test loss : 0.5833, accuracy : 80.51%\n","[31] test loss : 0.6781, accuracy : 77.79%\n","[32] test loss : 0.6268, accuracy : 77.81%\n","[33] test loss : 0.9051, accuracy : 72.30%\n","[34] test loss : 1.1378, accuracy : 67.65%\n","[35] test loss : 0.7089, accuracy : 77.71%\n","[36] test loss : 0.6113, accuracy : 79.05%\n","[37] test loss : 0.7254, accuracy : 76.15%\n","[38] test loss : 0.6609, accuracy : 78.71%\n","[39] test loss : 0.7150, accuracy : 77.49%\n","[40] test loss : 0.7157, accuracy : 76.86%\n","[41] test loss : 0.6519, accuracy : 78.73%\n","[42] test loss : 0.6633, accuracy : 77.30%\n","[43] test loss : 0.6899, accuracy : 76.28%\n","[44] test loss : 0.8825, accuracy : 73.25%\n","[45] test loss : 0.5523, accuracy : 81.67%\n","[46] test loss : 0.7063, accuracy : 76.72%\n","[47] test loss : 0.7994, accuracy : 74.47%\n","[48] test loss : 0.9336, accuracy : 71.68%\n","[49] test loss : 0.5418, accuracy : 81.60%\n","[50] test loss : 0.3396, accuracy : 88.31%\n","[51] test loss : 0.3325, accuracy : 88.62%\n","[52] test loss : 0.3227, accuracy : 89.04%\n","[53] test loss : 0.3190, accuracy : 88.94%\n","[54] test loss : 0.3196, accuracy : 89.21%\n","[55] test loss : 0.3150, accuracy : 89.38%\n","[56] test loss : 0.3110, accuracy : 89.39%\n","[57] test loss : 0.3176, accuracy : 89.43%\n","[58] test loss : 0.3311, accuracy : 89.06%\n","[59] test loss : 0.3090, accuracy : 89.74%\n","[60] test loss : 0.3299, accuracy : 89.23%\n","[61] test loss : 0.3217, accuracy : 89.23%\n","[62] test loss : 0.3206, accuracy : 89.51%\n","[63] test loss : 0.3301, accuracy : 89.22%\n","[64] test loss : 0.3283, accuracy : 89.22%\n","[65] test loss : 0.3292, accuracy : 89.01%\n","[66] test loss : 0.3129, accuracy : 89.60%\n","[67] test loss : 0.3345, accuracy : 88.93%\n","[68] test loss : 0.3320, accuracy : 89.15%\n","[69] test loss : 0.3391, accuracy : 89.11%\n","[70] test loss : 0.3474, accuracy : 88.51%\n","[71] test loss : 0.3802, accuracy : 88.08%\n","[72] test loss : 0.3641, accuracy : 88.11%\n","[73] test loss : 0.3584, accuracy : 88.01%\n","[74] test loss : 0.3766, accuracy : 88.20%\n","[75] test loss : 0.3364, accuracy : 89.40%\n","[76] test loss : 0.3827, accuracy : 87.54%\n","[77] test loss : 0.3393, accuracy : 88.91%\n","[78] test loss : 0.4030, accuracy : 87.45%\n","[79] test loss : 0.3391, accuracy : 88.99%\n","[80] test loss : 0.3537, accuracy : 88.87%\n","[81] test loss : 0.3635, accuracy : 88.39%\n","[82] test loss : 0.3926, accuracy : 87.73%\n","[83] test loss : 0.3673, accuracy : 87.97%\n","[84] test loss : 0.3965, accuracy : 87.29%\n","[85] test loss : 0.3590, accuracy : 88.70%\n","[86] test loss : 0.3805, accuracy : 88.41%\n","[87] test loss : 0.3743, accuracy : 88.39%\n","[88] test loss : 0.3546, accuracy : 88.66%\n","[89] test loss : 0.4003, accuracy : 87.44%\n","[90] test loss : 0.3791, accuracy : 88.10%\n","[91] test loss : 0.3806, accuracy : 87.88%\n","[92] test loss : 0.3820, accuracy : 87.69%\n","[93] test loss : 0.3795, accuracy : 88.06%\n","[94] test loss : 0.3767, accuracy : 87.95%\n","[95] test loss : 0.3624, accuracy : 88.35%\n","[96] test loss : 0.3743, accuracy : 88.06%\n","[97] test loss : 0.3759, accuracy : 88.34%\n","[98] test loss : 0.3663, accuracy : 88.37%\n","[99] test loss : 0.3877, accuracy : 87.83%\n","[100] test loss : 0.2830, accuracy : 90.78%\n","[101] test loss : 0.2771, accuracy : 90.80%\n","[102] test loss : 0.2768, accuracy : 90.88%\n","[103] test loss : 0.2784, accuracy : 90.98%\n","[104] test loss : 0.2769, accuracy : 90.86%\n","[105] test loss : 0.2785, accuracy : 90.74%\n","[106] test loss : 0.2748, accuracy : 91.03%\n","[107] test loss : 0.2776, accuracy : 90.83%\n","[108] test loss : 0.2800, accuracy : 90.95%\n","[109] test loss : 0.2758, accuracy : 91.01%\n","[110] test loss : 0.2811, accuracy : 90.84%\n","[111] test loss : 0.2806, accuracy : 91.07%\n","[112] test loss : 0.2833, accuracy : 90.90%\n","[113] test loss : 0.2816, accuracy : 90.88%\n","[114] test loss : 0.2824, accuracy : 90.87%\n","[115] test loss : 0.2806, accuracy : 90.90%\n","[116] test loss : 0.2859, accuracy : 90.94%\n","[117] test loss : 0.2871, accuracy : 90.74%\n","[118] test loss : 0.2863, accuracy : 90.86%\n","[119] test loss : 0.2853, accuracy : 90.78%\n","[120] test loss : 0.2893, accuracy : 90.84%\n","[121] test loss : 0.2883, accuracy : 90.81%\n","[122] test loss : 0.2933, accuracy : 90.60%\n","[123] test loss : 0.2938, accuracy : 90.66%\n","[124] test loss : 0.2947, accuracy : 90.66%\n","[125] test loss : 0.2924, accuracy : 90.73%\n","[126] test loss : 0.2954, accuracy : 90.64%\n","[127] test loss : 0.2963, accuracy : 90.51%\n","[128] test loss : 0.2950, accuracy : 90.62%\n","[129] test loss : 0.2941, accuracy : 90.59%\n","[130] test loss : 0.2957, accuracy : 90.53%\n","[131] test loss : 0.2981, accuracy : 90.70%\n","[132] test loss : 0.2999, accuracy : 90.58%\n","[133] test loss : 0.2980, accuracy : 90.53%\n","[134] test loss : 0.3013, accuracy : 90.79%\n","[135] test loss : 0.3010, accuracy : 90.75%\n","[136] test loss : 0.2972, accuracy : 90.64%\n","[137] test loss : 0.3033, accuracy : 90.61%\n","[138] test loss : 0.2976, accuracy : 90.57%\n","[139] test loss : 0.3025, accuracy : 90.52%\n","[140] test loss : 0.3026, accuracy : 90.77%\n","[141] test loss : 0.3032, accuracy : 90.75%\n","[142] test loss : 0.3002, accuracy : 90.85%\n","[143] test loss : 0.3018, accuracy : 90.71%\n","[144] test loss : 0.3109, accuracy : 90.66%\n","[145] test loss : 0.3094, accuracy : 90.70%\n","[146] test loss : 0.3023, accuracy : 90.77%\n","[147] test loss : 0.3095, accuracy : 90.83%\n","[148] test loss : 0.3078, accuracy : 90.80%\n","[149] test loss : 0.3103, accuracy : 90.72%\n","[150] test loss : 0.3076, accuracy : 90.74%\n","[151] test loss : 0.3048, accuracy : 90.75%\n","[152] test loss : 0.3051, accuracy : 90.80%\n","[153] test loss : 0.3041, accuracy : 90.78%\n","[154] test loss : 0.3044, accuracy : 90.85%\n","[155] test loss : 0.3025, accuracy : 90.73%\n","[156] test loss : 0.3036, accuracy : 90.77%\n","[157] test loss : 0.3043, accuracy : 90.82%\n","[158] test loss : 0.3050, accuracy : 90.80%\n","[159] test loss : 0.3073, accuracy : 90.77%\n","[160] test loss : 0.3050, accuracy : 90.74%\n","[161] test loss : 0.3034, accuracy : 90.82%\n","[162] test loss : 0.3033, accuracy : 90.89%\n","[163] test loss : 0.3056, accuracy : 90.87%\n","[164] test loss : 0.3017, accuracy : 90.92%\n","[165] test loss : 0.3054, accuracy : 90.77%\n","[166] test loss : 0.3029, accuracy : 90.75%\n","[167] test loss : 0.3032, accuracy : 90.86%\n","[168] test loss : 0.3045, accuracy : 90.87%\n","[169] test loss : 0.3038, accuracy : 90.83%\n","[170] test loss : 0.3062, accuracy : 90.74%\n","[171] test loss : 0.3058, accuracy : 90.79%\n","[172] test loss : 0.3047, accuracy : 90.85%\n","[173] test loss : 0.3065, accuracy : 90.82%\n","[174] test loss : 0.3035, accuracy : 90.82%\n","[175] test loss : 0.3075, accuracy : 90.90%\n","[176] test loss : 0.3060, accuracy : 90.80%\n","[177] test loss : 0.3046, accuracy : 90.91%\n","[178] test loss : 0.3064, accuracy : 90.89%\n","[179] test loss : 0.3054, accuracy : 90.84%\n","[180] test loss : 0.3057, accuracy : 90.82%\n","[181] test loss : 0.3062, accuracy : 90.78%\n","[182] test loss : 0.3054, accuracy : 90.79%\n","[183] test loss : 0.3063, accuracy : 90.69%\n","[184] test loss : 0.3057, accuracy : 90.82%\n","[185] test loss : 0.3066, accuracy : 90.63%\n","[186] test loss : 0.3042, accuracy : 90.83%\n","[187] test loss : 0.3055, accuracy : 90.73%\n","[188] test loss : 0.3038, accuracy : 90.82%\n","[189] test loss : 0.3007, accuracy : 90.76%\n","[190] test loss : 0.3067, accuracy : 90.88%\n","[191] test loss : 0.3056, accuracy : 90.80%\n","[192] test loss : 0.3073, accuracy : 90.77%\n","[193] test loss : 0.3060, accuracy : 90.80%\n","[194] test loss : 0.3059, accuracy : 90.78%\n","[195] test loss : 0.3063, accuracy : 90.79%\n","[196] test loss : 0.3043, accuracy : 90.71%\n","[197] test loss : 0.3073, accuracy : 90.64%\n","[198] test loss : 0.3079, accuracy : 90.76%\n","[199] test loss : 0.3067, accuracy : 90.75%\n","[200] test loss : 0.3052, accuracy : 90.71%\n","[201] test loss : 0.3062, accuracy : 90.71%\n","[202] test loss : 0.3049, accuracy : 90.74%\n","[203] test loss : 0.3079, accuracy : 90.76%\n","[204] test loss : 0.3050, accuracy : 90.81%\n","[205] test loss : 0.3067, accuracy : 90.76%\n","[206] test loss : 0.3085, accuracy : 90.74%\n","[207] test loss : 0.3073, accuracy : 90.74%\n","[208] test loss : 0.3090, accuracy : 90.71%\n","[209] test loss : 0.3060, accuracy : 90.71%\n","[210] test loss : 0.3066, accuracy : 90.79%\n","[211] test loss : 0.3068, accuracy : 90.71%\n","[212] test loss : 0.3055, accuracy : 90.67%\n","[213] test loss : 0.3037, accuracy : 90.78%\n","[214] test loss : 0.3063, accuracy : 90.79%\n","[215] test loss : 0.3092, accuracy : 90.85%\n","[216] test loss : 0.3084, accuracy : 90.73%\n","[217] test loss : 0.3071, accuracy : 90.83%\n","[218] test loss : 0.3057, accuracy : 90.73%\n","[219] test loss : 0.3067, accuracy : 90.77%\n","[220] test loss : 0.3061, accuracy : 90.68%\n","[221] test loss : 0.3093, accuracy : 90.77%\n","[222] test loss : 0.3037, accuracy : 90.75%\n","[223] test loss : 0.3066, accuracy : 90.71%\n","[224] test loss : 0.3049, accuracy : 90.75%\n","[225] test loss : 0.3040, accuracy : 90.76%\n","[226] test loss : 0.3046, accuracy : 90.81%\n","[227] test loss : 0.3053, accuracy : 90.74%\n","[228] test loss : 0.3049, accuracy : 90.76%\n","[229] test loss : 0.3058, accuracy : 90.76%\n","[230] test loss : 0.3045, accuracy : 90.71%\n","[231] test loss : 0.3045, accuracy : 90.63%\n","[232] test loss : 0.3049, accuracy : 90.82%\n","[233] test loss : 0.3068, accuracy : 90.71%\n","[234] test loss : 0.3077, accuracy : 90.81%\n","[235] test loss : 0.3091, accuracy : 90.75%\n","[236] test loss : 0.3067, accuracy : 90.76%\n","[237] test loss : 0.3082, accuracy : 90.80%\n","[238] test loss : 0.3077, accuracy : 90.83%\n","[239] test loss : 0.3056, accuracy : 90.83%\n","[240] test loss : 0.3051, accuracy : 90.69%\n","[241] test loss : 0.3062, accuracy : 90.75%\n","[242] test loss : 0.3057, accuracy : 90.69%\n","[243] test loss : 0.3077, accuracy : 90.75%\n","[244] test loss : 0.3096, accuracy : 90.76%\n","[245] test loss : 0.3067, accuracy : 90.89%\n"],"name":"stdout"}]}]}